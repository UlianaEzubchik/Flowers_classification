{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78178688",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.applications import MobileNet\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e9f4e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_path = 'dataset/train'\n",
    "data_test_path = 'dataset/test'\n",
    "data_val_path = 'dataset/valid'\n",
    "img_width = 224\n",
    "img_height = 224\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "432525be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6552 files belonging to 102 classes.\n"
     ]
    }
   ],
   "source": [
    "data_train = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_train_path,\n",
    "    label_mode='categorical',\n",
    "    shuffle = True,\n",
    "    image_size = (img_width, img_height),\n",
    "    batch_size = 32,\n",
    "    validation_split = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e04c8a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 818 files belonging to 102 classes.\n"
     ]
    }
   ],
   "source": [
    "data_val = tf.keras.utils.image_dataset_from_directory(\n",
    "    data_val_path,\n",
    "    label_mode='categorical',\n",
    "    shuffle = True,\n",
    "    image_size = (img_width, img_height),\n",
    "    batch_size = 32,\n",
    "    validation_split = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "863814a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target labels from the datasets\n",
    "data_train_labels = np.concatenate([labels for images, labels in data_train], axis=0)\n",
    "data_val_labels = np.concatenate([labels for images, labels in data_val], axis=0)\n",
    "\n",
    "# Convert target labels to one-hot encoded format\n",
    "num_classes = len(data_train.class_names)\n",
    "data_train_targets = to_categorical(data_train_labels, num_classes=num_classes)\n",
    "data_val_targets = to_categorical(data_val_labels, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "163cb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "981f5ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(Dropout(0.2))\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(102, activation='softmax'))  # 102 classes for 102 Flower Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02dac9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d453bebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205 26\n"
     ]
    }
   ],
   "source": [
    "num_train_samples = len(data_train)\n",
    "num_validation_samples = len(data_val)\n",
    "epochs = 10\n",
    "print(num_train_samples, num_validation_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66e5cecb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m529s\u001b[0m 2s/step - accuracy: 0.6154 - loss: 1.8540 - val_accuracy: 0.3350 - val_loss: 4.4392\n",
      "Epoch 2/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m521s\u001b[0m 3s/step - accuracy: 0.9391 - loss: 0.2186 - val_accuracy: 0.7335 - val_loss: 1.0033\n",
      "Epoch 3/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m523s\u001b[0m 3s/step - accuracy: 0.9690 - loss: 0.1014 - val_accuracy: 0.8105 - val_loss: 0.7258\n",
      "Epoch 4/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 2s/step - accuracy: 0.9769 - loss: 0.0816 - val_accuracy: 0.6296 - val_loss: 1.5723\n",
      "Epoch 5/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 2s/step - accuracy: 0.9385 - loss: 0.2100 - val_accuracy: 0.7372 - val_loss: 1.1316\n",
      "Epoch 6/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m494s\u001b[0m 2s/step - accuracy: 0.9629 - loss: 0.1143 - val_accuracy: 0.7176 - val_loss: 1.1401\n",
      "Epoch 7/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m474s\u001b[0m 2s/step - accuracy: 0.9743 - loss: 0.0845 - val_accuracy: 0.8484 - val_loss: 0.7387\n",
      "Epoch 8/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 2s/step - accuracy: 0.9756 - loss: 0.0828 - val_accuracy: 0.6687 - val_loss: 1.6504\n",
      "Epoch 9/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m476s\u001b[0m 2s/step - accuracy: 0.9645 - loss: 0.1002 - val_accuracy: 0.7641 - val_loss: 1.0854\n",
      "Epoch 10/10\n",
      "\u001b[1m205/205\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m477s\u001b[0m 2s/step - accuracy: 0.9691 - loss: 0.0979 - val_accuracy: 0.7787 - val_loss: 1.0548\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(data_train, validation_data = data_val, epochs = epochs, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59a35a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 445ms/step - accuracy: 0.7868 - loss: 0.9807\n",
      "Test accuracy: [1.0548375844955444, 0.7787286043167114]\n"
     ]
    }
   ],
   "source": [
    "test_acc = model.evaluate(data_val, steps=num_validation_samples // batch_size)\n",
    "print('Test accuracy:', test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a6b0768",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Flowers_classifier.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
